# Метод конечных компонент

Ещё одно решение проблемы мультиколлинеарности заключается в том, чтобы подвергнуть исходные признаки некоторому функциональному преобразованию, гарантировав линейную независимость новых признаков, и, возможно, сократив их количество, то есть уменьшив размерность задачи.

В методе главных компонент (principal component analysis, PCA) строится минимальное число новых признаков, по которым исходные признаки восстанавливаются линейным преобразованием с минимальными погрешностями. PCA относится к методам обучения без учителя (unsupervised learning), поскольку матрица «объекты–признаки» _F_ преобразуется без учёта целевого вектора _y_.

Постановка задачи. Пусть имеется n числовых признаков ![f_j (x), j = 1,\dots,n](https://i.upmath.me/svg/f_j%20(x)%2C%20j%20%3D%201%2C%5Cdots%2Cn). Как обычно, будем отождествлять объекты обучающей выборки и их признаковые описания: ![x_i ≡ (f_1(x_i),\dots,f_n(x_i)), i = 1,\dots, ℓ.](https://i.upmath.me/svg/x_i%20%E2%89%A1%20(f_1(x_i)%2C%5Cdots%2Cf_n(x_i))%2C%20i%20%3D%201%2C%5Cdots%2C%20%E2%84%93.) Рассмотрим матрицу F, строки которой соответствуют признаковым описаниям обучающих объектов: ![F_{l\times n} = \begin{pmatrix}f_1(x_1)&\dots&f_n(x_1)\\\dots&\dots&\dots\\f_1(x_l)&\dots&f_n(x_l)\end{pmatrix} =  \begin{pmatrix}x_{1}\\\dots\\x_{l}\end{pmatrix}](https://i.upmath.me/svg/F_%7Bl%5Ctimes%20n%7D%20%3D%20%5Cbegin%7Bpmatrix%7Df_1(x_1)%26%5Cdots%26f_n(x_1)%5C%5C%5Cdots%26%5Cdots%26%5Cdots%5C%5Cf_1(x_l)%26%5Cdots%26f_n(x_l)%5Cend%7Bpmatrix%7D%20%3D%20%20%5Cbegin%7Bpmatrix%7Dx_%7B1%7D%5C%5C%5Cdots%5C%5Cx_%7Bl%7D%5Cend%7Bpmatrix%7D)

Обозначим через ![z_i = (g_1(x_i),\dots,g_m(x_i))](https://i.upmath.me/svg/z_i%20%3D%20(g_1(x_i)%2C%5Cdots%2Cg_m(x_i))) признаковые описания тех же объектов в новом пространстве ![Z = \mathbb{R}_m](https://i.upmath.me/svg/Z%20%3D%20%5Cmathbb%7BR%7D_m) меньшей размерности, ![m<n](https://i.upmath.me/svg/m%3Cn): ![G_{l\times M} = \begin{pmatrix}g_1(x_1)&\dots&g_m(x_1)\\\dots&\dots&\dots\\g_1(x_l)&\dots&g_m(x_l)\end{pmatrix} =  \begin{pmatrix}z_{1}\\\dots\\z_{l}\end{pmatrix}](https://i.upmath.me/svg/G_%7Bl%5Ctimes%20M%7D%20%3D%20%5Cbegin%7Bpmatrix%7Dg_1(x_1)%26%5Cdots%26g_m(x_1)%5C%5C%5Cdots%26%5Cdots%26%5Cdots%5C%5Cg_1(x_l)%26%5Cdots%26g_m(x_l)%5Cend%7Bpmatrix%7D%20%3D%20%20%5Cbegin%7Bpmatrix%7Dz_%7B1%7D%5C%5C%5Cdots%5C%5Cz_%7Bl%7D%5Cend%7Bpmatrix%7D)

Потребуем, чтобы исходные признаковые описания можно было восстановить по новым описаниям с помощью некоторого линейного преобразования, определяемого матрицей ![U = (u_{js})_{n\times m}](https://i.upmath.me/svg/U%20%3D%20(u_%7Bjs%7D)_%7Bn%5Ctimes%20m%7D): ![\widehat{f}_j(x) =  \sum_{s=1}^{m}g_s(x)u_{js}, \ j = 1,\dots, n, \ x  \in X](https://i.upmath.me/svg/%5Cwidehat%7Bf%7D_j(x)%20%3D%20%20%5Csum_%7Bs%3D1%7D%5E%7Bm%7Dg_s(x)u_%7Bjs%7D%2C%20%5C%20j%20%3D%201%2C%5Cdots%2C%20n%2C%20%5C%20x%20%20%5Cin%20X)

или в векторной записи: ![\widehat{x} = zU^T](https://i.upmath.me/svg/%5Cwidehat%7Bx%7D%20%3D%20zU%5ET). Восстановленное описание ![\widehat{x}](https://i.upmath.me/svg/%5Cwidehat%7Bx%7D) не обязано в точности совпадать с исходным описанием ![x](https://i.upmath.me/svg/x), но их отличие на объектах обучающей выборки должно быть как можно меньше при выбранной размерности ![m](https://i.upmath.me/svg/m). Будем искать одновременно и матрицу новых признаковых описаний ![G](https://i.upmath.me/svg/G), и матрицу линейного преобразования ![U](https://i.upmath.me/svg/U), при которых суммарная невязка восстановленных описаний минимальна: ![\Delta^2(G,U) =  \sum_{i=1}^{l} ||\widehat{x}_i - x_i||^2 = \sum_{i=1}^{l} ||z_iU^T - x_i||^2 = ||GU^T - F||^2 \rightarrow \min_{G,U}](https://i.upmath.me/svg/%5CDelta%5E2(G%2CU)%20%3D%20%20%5Csum_%7Bi%3D1%7D%5E%7Bl%7D%20%7C%7C%5Cwidehat%7Bx%7D_i%20-%20x_i%7C%7C%5E2%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bl%7D%20%7C%7Cz_iU%5ET%20-%20x_i%7C%7C%5E2%20%3D%20%7C%7CGU%5ET%20-%20F%7C%7C%5E2%20%5Crightarrow%20%5Cmin_%7BG%2CU%7D)

где все нормы евклидовы. Напомним, что ![||A||^2 = trAA^T = tr A^TA](https://i.upmath.me/svg/%7C%7CA%7C%7C%5E2%20%3D%20trAA%5ET%20%3D%20tr%20A%5ETA), где ![tr](https://i.upmath.me/svg/tr) — операция следа матрицы.