# Метод конечных компонент

Ещё одно решение проблемы мультиколлинеарности заключается в том, чтобы подвергнуть исходные признаки некоторому функциональному преобразованию, гарантировав линейную независимость новых признаков, и, возможно, сократив их количество, то есть уменьшив размерность задачи.

В методе главных компонент (principal component analysis, PCA) строится минимальное число новых признаков, по которым исходные признаки восстанавливаются линейным преобразованием с минимальными погрешностями. PCA относится к методам обучения без учителя (unsupervised learning), поскольку матрица «объекты–признаки» *F* преобразуется без учёта целевого вектора *y*.

Постановка задачи. Пусть имеется n числовых признаков $$f_j (x), j = 1,\dots,n$$. Как обычно, будем отождествлять объекты обучающей выборки и их признаковые описания: $$x_i ≡ (f_1(x_i),\dots,f_n(x_i)), i = 1,\dots, ℓ.$$ Рассмотрим матрицу F, строки которой соответствуют признаковым описаниям обучающих объектов:
$$F_{l\times n} = \begin{pmatrix}f_1(x_1)&\dots&f_n(x_1)\\\dots&\dots&\dots\\f_1(x_l)&\dots&f_n(x_l)\end{pmatrix} =  \begin{pmatrix}x_{1}\\\dots\\x_{l}\end{pmatrix}$$

Обозначим через $$z_i = (g_1(x_i),\dots,g_m(x_i))$$ признаковые описания тех же объектов в новом пространстве $$Z = \mathbb{R}_m$$ меньшей размерности, $$m<n$$:
$$G_{l\times M} = \begin{pmatrix}g_1(x_1)&\dots&g_m(x_1)\\\dots&\dots&\dots\\g_1(x_l)&\dots&g_m(x_l)\end{pmatrix} =  \begin{pmatrix}z_{1}\\\dots\\z_{l}\end{pmatrix}$$

Потребуем, чтобы исходные признаковые описания можно было восстановить по новым описаниям с помощью некоторого линейного преобразования, определяемого матрицей $$U = (u_{js})_{n\times m}$$:
 $$\widehat{f}_j(x) =  \sum_{s=1}^{m}g_s(x)u_{js}, \ j = 1,\dots, n, \ x  \in X$$

или в векторной записи: $$\widehat{x} = zU^T$$. Восстановленное описание $$\widehat{x}$$ не обязано в точности совпадать с исходным описанием $$x$$, но их отличие на объектах обучающей выборки должно быть как можно меньше при выбранной размерности $$m$$. Будем искать одновременно и матрицу новых признаковых описаний $$G$$, и матрицу линейного преобразования $$U$$, при которых суммарная невязка восстановленных описаний минимальна:
 $$\Delta^2(G,U) =  \sum_{i=1}^{l} ||\widehat{x}_i - x_i||^2 = \sum_{i=1}^{l} ||z_iU^T - x_i||^2 = ||GU^T - F||^2 \rightarrow \min_{G,U}$$
 
где все нормы евклидовы. Напомним, что $$||A||^2 = trAA^T = tr A^TA$$, где $$tr$$ — операция следа матрицы.